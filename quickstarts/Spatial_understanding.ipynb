{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb5yiH5h8x3h"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YXR7Yn480fU"
      },
      "source": [
        "# 2D spatial understanding with Gemini 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeJyB7rG82ph"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "___eV40o8399"
      },
      "source": [
        "This notebook introduces object detection and spatial understanding with the Gemini API like in the [Spatial understanding example](https://aistudio.google.com/starter-apps/spatial) from [AI Studio](https://aistudio.google.com) and demonstrated in the [Building with Gemini 2.0: Spatial understanding](https://www.youtube.com/watch?v=-XmoDzDMqj4) video.\n",
        "\n",
        "You'll learn how to use Gemini the same way as in the demo and perform object detection like this:\n",
        "<img src=\"https://storage.googleapis.com/generativeai-downloads/images/cupcakes_with_bbox.png\" />\n",
        "\n",
        "There are many examples, including object detection with\n",
        "\n",
        "* simply overlaying information\n",
        "* searching within an image\n",
        "* translating and understanding things in multiple languages\n",
        "* using Gemini thinking abilities\n",
        "\n",
        "**Note**\n",
        "\n",
        "There's no \"magical prompt\". Feel free to experiment with different ones. You can use the dropdown to see different samples, but you can also write your own prompts. Also, you can try uploading your own images.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4dbK2FbXCJE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o6sel20XCJP"
      },
      "source": [
        "### Install SDK\n",
        "\n",
        "The new **[Google Gen AI SDK](https://github.com/googleapis/python-genai)** provides programmatic access to Gemini 2.0 (and previous models) using both the [Google AI for Developers](https://ai.google.dev/gemini-api/docs/models/gemini-v2) and [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview) APIs. With a few exceptions, code that runs on one platform will run on both. This means that you can prototype an application using the Developer API and then migrate the application to Vertex AI without rewriting your code.\n",
        "\n",
        "More details about this new SDK on the [documentation](https://googleapis.github.io/python-genai/) or in the [Getting started](../quickstarts/Get_started.ipynb) notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u53CdzCFXCJP"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsNLA8_YXCJQ"
      },
      "source": [
        "### Setup your API key\n",
        "\n",
        "To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TaPzmJ07XCJQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hx_Gw9i0Yuv"
      },
      "source": [
        "### Initialize SDK client\n",
        "\n",
        "With the new SDK you now only need to initialize a client with your API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HghvVpbU0Uap"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eW_IDAkXCJQ"
      },
      "source": [
        "### Select and configure a model\n",
        "\n",
        "Spatial understanding works best [Gemini 2.0 Flash model](https://ai.google.dev/gemini-api/docs/models/gemini-v2). It's even better with 2.5 models like `gemini-2.5-pro-exp-03-25` but slightly slower as it's a [thinking](./Get_started_thinking.ipynb) model.\n",
        "\n",
        "Some features, like segmentation, only works with 2.5 models.\n",
        "\n",
        "You can try with the older ones but it might be more inconsistent (`gemini-1.5-flash-001` had the best results of the previous generation). The [Object detection](https://github.com/google-gemini/cookbook/blob/gemini-1.5-archive/examples/Object_detection.ipynb) contains good examples of what previous models were able to do.\n",
        "\n",
        "For more information about all Gemini models, check the [documentation](https://ai.google.dev/gemini-api/docs/models/gemini) for extended information on each of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ydWzoN9nXCJQ"
      },
      "outputs": [],
      "source": [
        "model_name = \"gemini-2.5-pro-exp-03-25\" # @param [\"gemini-1.5-flash-latest\",\"gemini-2.0-flash-lite\",\"gemini-2.0-flash\",\"gemini-2.5-pro-exp-03-25\"] {\"allow-input\":true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RuQjPJ07Klh"
      },
      "source": [
        "### System instructions\n",
        "\n",
        " With the new SDK, the `system_instructions` and the `model` parameters must be passed in all `generate_content` calls, so let's save them to not have to type them all the time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lz1yU91P7IfN"
      },
      "outputs": [],
      "source": [
        "bounding_box_system_instructions = \"\"\"\n",
        "    Return bounding boxes as a JSON array with labels. Never return masks or code fencing. Limit to 25 objects.\n",
        "    If an object is present multiple times, name them according to their unique characteristic (colors, size, position, unique characteristics, etc..).\n",
        "      \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LtEryXuFDwRH"
      },
      "outputs": [],
      "source": [
        "safety_settings = [\n",
        "    types.SafetySetting(\n",
        "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        threshold=\"BLOCK_ONLY_HIGH\",\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufl4g-cUuR9E"
      },
      "source": [
        "The system instructions are mainly used to make the prompts shorter by not having to reapeat each time the format. They are also telling the model how to deal with similar objects which is a nice way to let it be creative.\n",
        "\n",
        "The [Spatial understanding example](https://aistudio.google.com/starter-apps/spatial) is using a different strategy with no system instructions but a longer prompt. You can see their full prompts by clicking on the \"show raw prompt\" button on the right. There no optimal solution, experiment with diffrent strategies and find the one that suits your use-case the best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOOZsm7i9io6"
      },
      "source": [
        "### Import\n",
        "\n",
        "Import all the necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aBLGCq09GIe"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from PIL import Image\n",
        "\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4Vx-ZbZCqWf"
      },
      "source": [
        "### Utils\n",
        "\n",
        "Some scripts will be needed to draw the bounding boxes. Of course they are just examples and you are free to just write your own.\n",
        "\n",
        "For example the [Spatial understanding example](https://aistudio.google.com/starter-apps/spatial) from [AI Studio](https://aistudio.google.com) uses HML to render the bounding boxes. You can find its code in the [Github repo](https://github.com/google-gemini/starter-applets/tree/main/spatial)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cHbhdPsH1PAS"
      },
      "outputs": [],
      "source": [
        "# @title Parsing JSON output\n",
        "def parse_json(json_output: str):\n",
        "    # Parsing out the markdown fencing\n",
        "    lines = json_output.splitlines()\n",
        "    for i, line in enumerate(lines):\n",
        "        if line == \"```json\":\n",
        "            json_output = \"\\n\".join(lines[i+1:])  # Remove everything before \"```json\"\n",
        "            json_output = json_output.split(\"```\")[0]  # Remove everything after the closing \"```\"\n",
        "            break  # Exit the loop once \"```json\" is found\n",
        "    return json_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oxK0pycZm4AY"
      },
      "outputs": [],
      "source": [
        "# @title Plotting Util\n",
        "\n",
        "# Get Noto JP font to display janapese characters\n",
        "!apt-get install fonts-noto-cjk  # For Noto Sans CJK JP\n",
        "\n",
        "#!apt-get install fonts-source-han-sans-jp # For Source Han Sans (Japanese)\n",
        "\n",
        "import json\n",
        "import random\n",
        "import io\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from PIL import ImageColor\n",
        "\n",
        "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
        "\n",
        "def plot_bounding_boxes(im, bounding_boxes):\n",
        "    \"\"\"\n",
        "    Plots bounding boxes on an image with markers for each a name, using PIL, normalized coordinates, and different colors.\n",
        "\n",
        "    Args:\n",
        "        img_path: The path to the image file.\n",
        "        bounding_boxes: A list of bounding boxes containing the name of the object\n",
        "         and their positions in normalized [y1 x1 y2 x2] format.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the image\n",
        "    img = im\n",
        "    width, height = img.size\n",
        "    print(img.size)\n",
        "    # Create a drawing object\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    # Define a list of colors\n",
        "    colors = [\n",
        "    'red',\n",
        "    'green',\n",
        "    'blue',\n",
        "    'yellow',\n",
        "    'orange',\n",
        "    'pink',\n",
        "    'purple',\n",
        "    'brown',\n",
        "    'gray',\n",
        "    'beige',\n",
        "    'turquoise',\n",
        "    'cyan',\n",
        "    'magenta',\n",
        "    'lime',\n",
        "    'navy',\n",
        "    'maroon',\n",
        "    'teal',\n",
        "    'olive',\n",
        "    'coral',\n",
        "    'lavender',\n",
        "    'violet',\n",
        "    'gold',\n",
        "    'silver',\n",
        "    ] + additional_colors\n",
        "\n",
        "    # Parsing out the markdown fencing\n",
        "    bounding_boxes = parse_json(bounding_boxes)\n",
        "\n",
        "    font = ImageFont.truetype(\"NotoSansCJK-Regular.ttc\", size=14)\n",
        "\n",
        "    # Iterate over the bounding boxes\n",
        "    for i, bounding_box in enumerate(json.loads(bounding_boxes)):\n",
        "      # Select a color from the list\n",
        "      color = colors[i % len(colors)]\n",
        "\n",
        "      # Convert normalized coordinates to absolute coordinates\n",
        "      abs_y1 = int(bounding_box[\"box_2d\"][0]/1000 * height)\n",
        "      abs_x1 = int(bounding_box[\"box_2d\"][1]/1000 * width)\n",
        "      abs_y2 = int(bounding_box[\"box_2d\"][2]/1000 * height)\n",
        "      abs_x2 = int(bounding_box[\"box_2d\"][3]/1000 * width)\n",
        "\n",
        "      if abs_x1 > abs_x2:\n",
        "        abs_x1, abs_x2 = abs_x2, abs_x1\n",
        "\n",
        "      if abs_y1 > abs_y2:\n",
        "        abs_y1, abs_y2 = abs_y2, abs_y1\n",
        "\n",
        "      # Draw the bounding box\n",
        "      draw.rectangle(\n",
        "          ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4\n",
        "      )\n",
        "\n",
        "      # Draw the text\n",
        "      if \"label\" in bounding_box:\n",
        "        draw.text((abs_x1 + 8, abs_y1 + 6), bounding_box[\"label\"], fill=color, font=font)\n",
        "\n",
        "    # Display the image\n",
        "    img.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqPO819ysM3c"
      },
      "source": [
        "### Get example images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pb2qWTIqsImv"
      },
      "outputs": [],
      "source": [
        "# Load sample images\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/socks.jpg -O Socks.jpg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/vegetables.jpg -O Vegetables.jpg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/Japanese_Bento.png -O Japanese_bento.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/Cupcakes.jpg -O Cupcakes.jpg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/origamis.jpg -O Origamis.jpg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/fruits.jpg -O Fruits.jpg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/cat.jpg -O Cat.jpg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/pumpkins.jpg -O Pumpkins.jpg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/breakfast.jpg -O Breakfast.jpg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/bookshelf.jpg -O Bookshelf.jpg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/spill.jpg -O Spill.jpg -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFLDgSztv77H"
      },
      "source": [
        "## Overlaying Information\n",
        "\n",
        "Let's start by loading an image, the origami one for example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSFBHGJjxIJb"
      },
      "outputs": [],
      "source": [
        "image = \"Cupcakes.jpg\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true}\n",
        "\n",
        "im = Image.open(image)\n",
        "im.thumbnail([620,620], Image.Resampling.LANCZOS)\n",
        "im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYfXLLgRyDfo"
      },
      "source": [
        "Let's start with a simple prompt to find all items in the image.\n",
        "\n",
        "To prevent the model from repeating itself, it is recommended to use a temperature over 0, in this case 0.5. Limiting the number of items (25 in the systemp instructions) is also a way to prevent the model from looping and to speed up the decoding of the bounding boxes. You can experiment with these parameters and find what works best for your use-case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phxOEE6Gx_Vy"
      },
      "outputs": [],
      "source": [
        "prompt = \"Detect the 2d bounding boxes of the cupcakes (with “label” as topping description”)\"  # @param {type:\"string\"}\n",
        "\n",
        "# Load and resize image\n",
        "im = Image.open(BytesIO(open(image, \"rb\").read()))\n",
        "im.thumbnail([1024,1024], Image.Resampling.LANCZOS)\n",
        "\n",
        "# Run model to find bounding boxes\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[prompt, im],\n",
        "    config = types.GenerateContentConfig(\n",
        "        system_instruction=bounding_box_system_instructions,\n",
        "        temperature=0.5,\n",
        "        safety_settings=safety_settings,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Check output\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxlObJhF00mR"
      },
      "source": [
        "As you can see, even without any instructions about the format, Gemini is trained to always use this format with a label and the coordinates of the bounding box in a \"box_2d\" array.\n",
        "\n",
        "Just be careful, the y coordinates are first, x ones afterwards contrary to common usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZBNvc1XzJP5"
      },
      "outputs": [],
      "source": [
        "plot_bounding_boxes(im, response.text)\n",
        "im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjP8ktS62QRv"
      },
      "source": [
        "## Search within an image\n",
        "\n",
        "Let's complicate things and search within the image for specific objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlF_8H2f2ZEI"
      },
      "outputs": [],
      "source": [
        "image = \"Socks.jpg\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true}\n",
        "prompt = \"Show me the positions of the socks with the face\"  # @param [\"Detect all rainbow socks\", \"Find all socks and label them with emojis \", \"Show me the positions of the socks with the face\",\"Find the sock that goes with the one at the top\"] {\"allow-input\":true}\n",
        "\n",
        "# Load and resize image\n",
        "im = Image.open(image)\n",
        "im.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "\n",
        "# Run model to find bounding boxes\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[prompt, im],\n",
        "    config = types.GenerateContentConfig(\n",
        "        system_instruction=bounding_box_system_instructions,\n",
        "        temperature=0.5,\n",
        "        safety_settings=safety_settings,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Check output\n",
        "print(response.text)\n",
        "\n",
        "# Generate image with bounding boxes\n",
        "plot_bounding_boxes(im, response.text)\n",
        "im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SctVyhDFMOK"
      },
      "source": [
        "Try it with different images and prompts. Different samples are proposed but you can also write your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvVSSr7z3uN4"
      },
      "source": [
        "## Multilinguality\n",
        "\n",
        "As Gemini is able to understand multiple languages, you can combine spatial reasoning with multilingual capabilities.\n",
        "\n",
        "You can give it an image like this and prompt it to label each item with Japanese characters and English translation. The model reads the text and recognize the pictures from the image itself and translates them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLmbPUTg3wxx"
      },
      "outputs": [],
      "source": [
        "image = \"Japanese_bento.png\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true}\n",
        "prompt = \"Detect food, label them with Japanese characters + english translation.\"  # @param [\"Detect food, label them with Japanese characters + english translation.\", \"Show me the vegan dishes\",\"Explain what those dishes are with a 5 words description\",\"Find the dishes with allergens and label them accordingly\"] {\"allow-input\":true}\n",
        "\n",
        "# Load and resize image\n",
        "im = Image.open(image)\n",
        "im.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "\n",
        "# Run model to find bounding boxes\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[prompt, im],\n",
        "    config = types.GenerateContentConfig(\n",
        "        system_instruction=bounding_box_system_instructions,\n",
        "        temperature=0.5,\n",
        "        safety_settings=safety_settings,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Generate image with bounding boxes\n",
        "plot_bounding_boxes(im, response.text)\n",
        "im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZbhjYkUA86w"
      },
      "source": [
        "## Use Gemini reasoning capabilities\n",
        "\n",
        "The model can also reason based on the image, you can ask it about the positions of items, their utility, or, like in this example, to find the shadow of a speficic item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-IHhnxgBNdD"
      },
      "outputs": [],
      "source": [
        "image = \"Origamis.jpg\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true}\n",
        "prompt = \"Draw a square around the fox' shadow\"  # @param [\"Find the two origami animals.\", \"Where are the origamis' shadows?\",\"Draw a square around the fox' shadow\"] {\"allow-input\":true}\n",
        "\n",
        "# Load and resize image\n",
        "im = Image.open(image)\n",
        "im.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "\n",
        "# Run model to find bounding boxes\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[prompt, im],\n",
        "    config = types.GenerateContentConfig(\n",
        "        system_instruction=bounding_box_system_instructions,\n",
        "        temperature=0.5,\n",
        "        safety_settings=safety_settings,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Generate image with bounding boxes\n",
        "plot_bounding_boxes(im, response.text)\n",
        "im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un65SR4JRv2s"
      },
      "source": [
        "You can also use Gemini knowledge to enhanced the labels returned. In this example Gemini will give you advices on how to fix your little mistake.\n",
        "\n",
        "As you can see this time, you're only resizing the image to 1024px as it helps the model getting the bigger picture adn give you advices. There's no clear rule about when to do it, experiment and find what works the best for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz6_AcMNRvEm"
      },
      "outputs": [],
      "source": [
        "image = \"Spill.jpg\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true}\n",
        "prompt = \"Tell me how to clean my table with an explanation as label. Do not just label the items\"  # @param [\"Show me where my coffee was spilled.\", \"Tell me how to clean my table with an explanation as label. Do not just label the items\",\"Draw a square around the fox' shadow\"] {\"allow-input\":true}\n",
        "\n",
        "# Load and resize image\n",
        "im = Image.open(image)\n",
        "im.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
        "\n",
        "# Run model to find bounding boxes\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[prompt, im],\n",
        "    config = types.GenerateContentConfig(\n",
        "        system_instruction=bounding_box_system_instructions,\n",
        "        temperature=0.5,\n",
        "        safety_settings=safety_settings,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Generate image with bounding boxes\n",
        "plot_bounding_boxes(im, response.text)\n",
        "im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1YTKxQk8imS"
      },
      "outputs": [],
      "source": [
        "response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQszSykaTBHo"
      },
      "source": [
        "And if you check the previous examples, the [Japanese food](#scrollTo=tvVSSr7z3uN4) one in particular, multiple other prompt samples are provided to experiment with Gemini reasoning capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQJTJ8wdGOKx"
      },
      "source": [
        "## Experimental: Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PDzWh9AGjl-"
      },
      "source": [
        "2.5 models are also able to segment the image and not only draw a bounding box but to also provide a mask of the contour of the items. It's especially useful if you are planning on editing images like in the [Virtual try-on](../examples/Virtual_Try_On.ipynb) example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Oz8dSKbeB1J2"
      },
      "outputs": [],
      "source": [
        "# @title Segmentation Utils\n",
        "\n",
        "import dataclasses\n",
        "import numpy as np\n",
        "import base64\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class SegmentationMask:\n",
        "  # bounding box pixel coordinates (not normalized)\n",
        "  y0: int # in [0..height - 1]\n",
        "  x0: int # in [0..width - 1]\n",
        "  y1: int # in [0..height - 1]\n",
        "  x1: int # in [0..width - 1]\n",
        "  mask: np.array # [img_height, img_width] with values 0..255\n",
        "  label: str\n",
        "\n",
        "def parse_segmentation_masks(\n",
        "    predicted_str: str, *, img_height: int, img_width: int\n",
        ") -> list[SegmentationMask]:\n",
        "  items = json.loads(parse_json(predicted_str))\n",
        "  masks = []\n",
        "  for item in items:\n",
        "    raw_box = item[\"box_2d\"]\n",
        "    abs_y0 = int(item[\"box_2d\"][0] / 1000 * img_height)\n",
        "    abs_x0 = int(item[\"box_2d\"][1] / 1000 * img_width)\n",
        "    abs_y1 = int(item[\"box_2d\"][2] / 1000 * img_height)\n",
        "    abs_x1 = int(item[\"box_2d\"][3] / 1000 * img_width)\n",
        "    if abs_y0 >= abs_y1 or abs_x0 >= abs_x1:\n",
        "      print(\"Invalid bounding box\", item[\"box_2d\"])\n",
        "      continue\n",
        "    label = item[\"label\"]\n",
        "    png_str = item[\"mask\"]\n",
        "    if not png_str.startswith(\"data:image/png;base64,\"):\n",
        "      print(\"Invalid mask\")\n",
        "      continue\n",
        "    png_str = png_str.removeprefix(\"data:image/png;base64,\")\n",
        "    png_str = base64.b64decode(png_str)\n",
        "    mask = Image.open(io.BytesIO(png_str))\n",
        "    bbox_height = abs_y1 - abs_y0\n",
        "    bbox_width = abs_x1 - abs_x0\n",
        "    if bbox_height < 1 or bbox_width < 1:\n",
        "      print(\"Invalid bounding box\")\n",
        "      continue\n",
        "    mask = mask.resize((bbox_width, bbox_height), resample=Image.Resampling.BILINEAR)\n",
        "    np_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
        "    np_mask[abs_y0:abs_y1, abs_x0:abs_x1] = mask\n",
        "    masks.append(SegmentationMask(abs_y0, abs_x0, abs_y1, abs_x1, np_mask, label))\n",
        "  return masks\n",
        "\n",
        "def overlay_mask_on_img(\n",
        "    img: Image,\n",
        "    mask: np.ndarray,\n",
        "    color: str,\n",
        "    alpha: float = 0.7\n",
        ") -> Image.Image:\n",
        "    \"\"\"\n",
        "    Overlays a single mask onto a PIL Image using a named color.\n",
        "\n",
        "    The mask image defines the area to be colored. Non-zero pixels in the\n",
        "    mask image are considered part of the area to overlay.\n",
        "\n",
        "    Args:\n",
        "        img: The base PIL Image object.\n",
        "        mask: A PIL Image object representing the mask.\n",
        "              Should have the same height and width as the img.\n",
        "              Modes '1' (binary) or 'L' (grayscale) are typical, where\n",
        "              non-zero pixels indicate the masked area.\n",
        "        color: A standard color name string (e.g., 'red', 'blue', 'yellow').\n",
        "        alpha: The alpha transparency level for the overlay (0.0 fully\n",
        "               transparent, 1.0 fully opaque). Default is 0.7 (70%).\n",
        "\n",
        "    Returns:\n",
        "        A new PIL Image object (in RGBA mode) with the mask overlaid.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If color name is invalid, mask dimensions mismatch img\n",
        "                    dimensions, or alpha is outside the 0.0-1.0 range.\n",
        "    \"\"\"\n",
        "    if not (0.0 <= alpha <= 1.0):\n",
        "        raise ValueError(\"Alpha must be between 0.0 and 1.0\")\n",
        "\n",
        "    # Convert the color name string to an RGB tuple\n",
        "    try:\n",
        "        color_rgb: Tuple[int, int, int] = ImageColor.getrgb(color)\n",
        "    except ValueError as e:\n",
        "        # Re-raise with a more informative message if color name is invalid\n",
        "        raise ValueError(f\"Invalid color name '{color}'. Supported names are typically HTML/CSS color names. Error: {e}\")\n",
        "\n",
        "    # Prepare the base image for alpha compositing\n",
        "    img_rgba = img.convert(\"RGBA\")\n",
        "    width, height = img_rgba.size\n",
        "\n",
        "    # Create the colored overlay layer\n",
        "    # Calculate the RGBA tuple for the overlay color\n",
        "    alpha_int = int(alpha * 255)\n",
        "    overlay_color_rgba = color_rgb + (alpha_int,)\n",
        "\n",
        "    # Create an RGBA layer (all zeros = transparent black)\n",
        "    colored_mask_layer_np = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    # Mask has values between 0 and 255, threshold at 127 to get binary mask.\n",
        "    mask_np_logical = mask > 127\n",
        "\n",
        "    # Apply the overlay color RGBA tuple where the mask is True\n",
        "    colored_mask_layer_np[mask_np_logical] = overlay_color_rgba\n",
        "\n",
        "    # Convert the NumPy layer back to a PIL Image\n",
        "    colored_mask_layer_pil = Image.fromarray(colored_mask_layer_np, 'RGBA')\n",
        "\n",
        "    # Composite the colored mask layer onto the base image\n",
        "    result_img = Image.alpha_composite(img_rgba, colored_mask_layer_pil)\n",
        "\n",
        "    return result_img\n",
        "\n",
        "def plot_segmentation_masks(img: Image, segmentation_masks: list[SegmentationMask]):\n",
        "    \"\"\"\n",
        "    Plots bounding boxes on an image with markers for each a name, using PIL, normalized coordinates, and different colors.\n",
        "\n",
        "    Args:\n",
        "        img: The PIL.Image.\n",
        "        segmentation_masks: A string encoding as JSON a list of segmentation masks containing the name of the object,\n",
        "         their positions in normalized [y1 x1 y2 x2] format, and the png encoded segmentation mask.\n",
        "    \"\"\"\n",
        "    # Define a list of colors\n",
        "    colors = [\n",
        "    'red',\n",
        "    'green',\n",
        "    'blue',\n",
        "    'yellow',\n",
        "    'orange',\n",
        "    'pink',\n",
        "    'purple',\n",
        "    'brown',\n",
        "    'gray',\n",
        "    'beige',\n",
        "    'turquoise',\n",
        "    'cyan',\n",
        "    'magenta',\n",
        "    'lime',\n",
        "    'navy',\n",
        "    'maroon',\n",
        "    'teal',\n",
        "    'olive',\n",
        "    'coral',\n",
        "    'lavender',\n",
        "    'violet',\n",
        "    'gold',\n",
        "    'silver',\n",
        "    ] + additional_colors\n",
        "    font = ImageFont.truetype(\"NotoSansCJK-Regular.ttc\", size=14)\n",
        "\n",
        "    # Do this in 3 passes to make sure the boxes and text are always visible.\n",
        "\n",
        "    # Overlay the mask\n",
        "    for i, mask in enumerate(segmentation_masks):\n",
        "      color = colors[i % len(colors)]\n",
        "      img = overlay_mask_on_img(img, mask.mask, color)\n",
        "\n",
        "    # Create a drawing object\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    # Draw the bounding boxes\n",
        "    for i, mask in enumerate(segmentation_masks):\n",
        "      color = colors[i % len(colors)]\n",
        "      draw.rectangle(\n",
        "          ((mask.x0, mask.y0), (mask.x1, mask.y1)), outline=color, width=4\n",
        "      )\n",
        "\n",
        "    # Draw the text labels\n",
        "    for i, mask in enumerate(segmentation_masks):\n",
        "      color = colors[i % len(colors)]\n",
        "      if mask.label != \"\":\n",
        "        draw.text((mask.x0 + 8, mask.y0 - 20), mask.label, fill=color, font=font)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t-MOFZcGjl_"
      },
      "outputs": [],
      "source": [
        "image = \"Cupcakes.jpg\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true}\n",
        "prompt = \"Give the segmentation masks for the metal, wooden and glass small items (ignore the table). Output a JSON list of segmentation masks where each entry contains the 2D bounding box in the key \\\"box_2d\\\", the segmentation mask in key \\\"mask\\\", and the text label in the key \\\"label\\\". Use descriptive labels.\"  # @param {type:\"string\"}\n",
        "\n",
        "# Load and resize image\n",
        "im = Image.open(BytesIO(open(image, \"rb\").read()))\n",
        "im.thumbnail([1024,1024], Image.Resampling.LANCZOS)\n",
        "\n",
        "# Run model to find segmentation masks\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[prompt, im],\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature=0.5,\n",
        "        safety_settings=safety_settings,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Check output\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxHD1miyLWhK"
      },
      "source": [
        "The model predicts a JSON list, where each item represents a segmentation mask. Each item has a bounding box (\"`box_2d`\") in the format `[y0, x0, y1, x1]` with normalized coordinates between 0 and 1000, a label (\"`label`\") that identifies the object, and lastly the segmentation mask inside the bounding box, as base64 encoded png.\n",
        "\n",
        "To use the mask, first you need to do base64 decoding, and then loading this string as a png. This will give you a probability map with values between 0 and 255. The mask needs to be resized to match the bounding box dimensions, then you can apply your confidence threshold, e.g. binarizing at 127 for the midpoint. Finally, pad the mask into an array of the size of the full image.\n",
        "\n",
        "All these steps are done by the the `parse_segmentation_masks` function provided earlier.\n",
        "\n",
        "Ultimately, use the `plot_segmentation_masks` function to visualize the decoded masks by overlaying it on the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlHJHesPUJCs"
      },
      "outputs": [],
      "source": [
        "segmentation_masks = parse_segmentation_masks(response.text, img_height=im.size[1], img_width=im.size[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf6T89h34mXx"
      },
      "outputs": [],
      "source": [
        "plot_segmentation_masks(im, segmentation_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht_OJ5-Q9vfN"
      },
      "source": [
        "## Preliminary capabilities: pointing and 3D boxes\n",
        "\n",
        "Pointing and 3D bounding boxes are experimental model capabilities. Check this [other notebook](../examples/Spatial_understanding_3d.ipynb) to get a sneak peek on those upcoming capabilities.\n",
        "\n",
        "<a href=\"../examples/Spatial_understanding_3d.ipynb\"><img src=\"https://storage.googleapis.com/generativeai-downloads/images/box_3d.png\" height=\"400\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "245bc92a470f"
      },
      "source": [
        "## What's next?\n",
        "\n",
        "For a more end-to-end example, the code from the [AI Studio Spatial understanding example](https://aistudio.google.com/starter-apps/spatial)  is available on [Github](https://github.com/google-gemini/starter-applets/tree/main/spatial).\n",
        "\n",
        "You'll also find multiple other examples of Gemini 2.0 capabilities in the [Gemini 2.0 cookbook](https://github.com/google-gemini/cookbook/tree/main/gemini-2/), in particular the [Live API](./Get_started_LiveAPI.ipynb) and the [video understanding](./Video_understanding.ipynb) one.\n",
        "\n",
        "Related to image recognition and reasoning, [Market a jet backpack](../examples/Market_a_Jet_Backpack.ipynb) and [Guess the shape](../examples/Guess_the_shape.ipynb) examples are worth checking to continue your Gemini API discovery (Note: these examples still use the old SDK). And of course the [pointing and 3d boxes](../examples/Spatial_understanding_3d.ipynb) example referenced earlier."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Spatial_understanding.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}